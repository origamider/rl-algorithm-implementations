# DQN(Deep Q-Network)

## 概要
- Q学習と深層ニューラルネットワークを組み合わせた強化学習アルゴリズム。
- 通常、Q学習は、状態SとアクションAによって表現され、Qを求めようとすると、|S|*|A|のQテーブルが必要になる。=> 状態数と行動数が多くなると、実用的でない

### 将棋の例
- **状態数 |S|**: 10^70以上？
- **行動数 |A|**: 40手程度
- **必要なテーブルサイズ**:10^70*40 = 無限

## DQNの解決策
- ニューラルネットワークに通し、近似したQ値を取得。
- こうすることで、Qテーブルは不要になる。

## コア技術
- 経験再生
    - 通常の教師あり学習では、ミニバッチ学習などを通じて、データをランダムに取得して学習させる。
    - ただ、Q学習では $E_{t} = (S_{t},A_{t},R_{t},S_{t+1})$ のデータを用いてQ関数を更新するため、 $E_{t}$ と $E_{t+1}$ に強い相関が出てしまう。そこで、agentが経験したデータをBufferに保存し、Q関数の更新時に、そのBufferから経験データをランダムに取得することで、偏りのないデータでの学習を可能にする。
- ターゲットネットワーク
    - 通常の教師あり学習では、正解ラベルは固定されているが、Q学習だとQ値が逐次更新されるため、いつまで経ってもQ値が正解ラベルに近づかない現象に陥る。
    - そこで、TDターゲットを学習するオリジナルのネットワークqnetと、同じ構造のネットワークqnet_targetを用意する。qnet_targetはqnetと同期しており、一定期間重みを更新しないことで、一時的に正解ラベルを固定する役割がある。なお、TDターゲット全く更新しないと、Q関数の学習は進まないため、一定周期ごとにqnetからqnet_targetを更新する。