from collections import defaultdict,deque
import numpy as np

# 方策ON型SARSAの実装
class SarsaOnPolicyAgent:
    def __init__(self):
        self.gamma = 0.9
        self.alpha = 0.8
        self.epsilon = 0.1
        self.action_size = 4
        random_probs = {0:0.25,1:0.25,2:0.25,3:0.25}
        self.pi = defaultdict(lambda:random_probs)
        self.Q = defaultdict(lambda:0)
        self.memory = deque(maxlen=2) # {St,At,Rt,done_t},{S_(t+1),A_(t+1),R_(t+1),done_(t+1)}を保持。

    def get_action(self,state):
        action_probs = self.pi[state]
        actions = list(action_probs.keys())
        probs = list(action_probs.values())
        return np.random.choice(actions,p=probs)
    
    def reset(self):
        self.memory.clear()

    # その状態における、最適方策を求める。
    # Q関数の最も大きいactionを(1-ε)+ε/4,それ以外をε/4とする。(合計1)
    def greedy_probs(self,state,epsilon=0.0,action_size=4):
        qs = []
        for action in range(action_size):
            qs.append(self.Q[(state,action)])
        max_action = int(np.argmax(qs))
        base_prob = epsilon/action_size
        res ={action:base_prob for action in range(action_size)}
        res[max_action] += (1-epsilon)
        return res

    # Q関数の更新処理
    # Q'(St,At) = Q(St,At) + α{Rt + γQ(S_(t+1),A_(t+1)) - Q(St,At)}
    def update(self,state,action,reward,done):
        self.memory.append((state,action,reward,done))

        if len(self.memory) < 2:
            return
        
        state,action,reward,done = self.memory[0]
        next_state,next_action,_,_ = self.memory[1]
        next_q = 0 if done else self.Q[(next_state,next_action)]
        idx = (state,action)
        target = reward + self.gamma*next_q
        self.Q[idx] += self.alpha*(target-self.Q[idx])
        self.pi[state] = self.greedy_probs(state,self.epsilon,self.action_size)